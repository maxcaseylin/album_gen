{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "39bc0dc97671e1c9f2ba11d8a58f729db225ce60fed77ad6604919130d5e7393"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preliminary imports\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import time\n",
    "from tensorflow.keras import layers\n",
    "import cv2\n",
    "\n",
    "#for gif generation\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataset from ./training_data/\n",
    "#first define some parameters:\n",
    "#first start at 50 per batch, then move up\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "IMG_WIDTH = 128\n",
    "IMG_HEIGHT = 128\n",
    "\n",
    "train_dir = r'./test_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 1218 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "#create the training dataset\n",
    "#note that we do not use validation here since validation comes from the classifier, so all images are used for training here\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    image_size = (IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    label_mode = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(50, 128, 128, 3)\ntf.Tensor(\n[[[[ 53.  52.  48.]\n   [ 39.  38.  34.]\n   [ 56.  55.  51.]\n   ...\n   [ 43.  44.  39.]\n   [ 45.  46.  41.]\n   [ 48.  49.  44.]]\n\n  [[ 50.  49.  45.]\n   [ 52.  51.  47.]\n   [ 47.  46.  42.]\n   ...\n   [ 43.  44.  39.]\n   [ 45.  46.  41.]\n   [ 48.  49.  44.]]\n\n  [[ 44.  45.  40.]\n   [ 55.  56.  51.]\n   [ 37.  38.  33.]\n   ...\n   [ 43.  44.  39.]\n   [ 45.  46.  41.]\n   [ 48.  49.  44.]]\n\n  ...\n\n  [[ 47.  48.  43.]\n   [ 46.  47.  42.]\n   [ 46.  47.  42.]\n   ...\n   [ 47.  48.  43.]\n   [ 48.  49.  44.]\n   [ 49.  50.  45.]]\n\n  [[ 47.  48.  43.]\n   [ 46.  47.  42.]\n   [ 46.  47.  42.]\n   ...\n   [ 47.  48.  43.]\n   [ 48.  49.  44.]\n   [ 49.  50.  45.]]\n\n  [[ 47.  48.  43.]\n   [ 46.  47.  42.]\n   [ 46.  47.  42.]\n   ...\n   [ 47.  48.  43.]\n   [ 48.  49.  44.]\n   [ 49.  50.  45.]]]\n\n\n [[[115. 131. 130.]\n   [128. 140. 140.]\n   [152. 156. 159.]\n   ...\n   [158. 190. 201.]\n   [166. 194. 206.]\n   [191. 217. 230.]]\n\n  [[126. 138. 138.]\n   [123. 131. 133.]\n   [119. 118. 123.]\n   ...\n   [158. 190. 201.]\n   [158. 186. 198.]\n   [169. 195. 208.]]\n\n  [[102. 107. 111.]\n   [127. 128. 133.]\n   [154. 147. 155.]\n   ...\n   [186. 218. 229.]\n   [174. 202. 214.]\n   [162. 188. 201.]]\n\n  ...\n\n  [[116.  52.  42.]\n   [103.  45.  34.]\n   [ 66.  17.  12.]\n   ...\n   [ 90.   1.   3.]\n   [101.  26.  21.]\n   [ 72.   4.   0.]]\n\n  [[117.  55.  44.]\n   [ 80.  23.  12.]\n   [105.  58.  52.]\n   ...\n   [106.  14.  15.]\n   [131.  54.  48.]\n   [ 82.  12.   2.]]\n\n  [[ 77.  15.   4.]\n   [ 88.  31.  20.]\n   [129.  82.  76.]\n   ...\n   [117.  25.  26.]\n   [149.  70.  65.]\n   [ 88.  18.   8.]]]\n\n\n [[[255. 255. 255.]\n   [255. 255. 255.]\n   [255. 255. 255.]\n   ...\n   [255. 255. 255.]\n   [255. 255. 255.]\n   [255. 255. 255.]]\n\n  [[255. 255. 255.]\n   [255. 255. 255.]\n   [255. 255. 255.]\n   ...\n   [255. 255. 255.]\n   [255. 255. 255.]\n   [255. 255. 255.]]\n\n  [[255. 255. 255.]\n   [255. 255. 255.]\n   [255. 255. 255.]\n   ...\n   [255. 255. 255.]\n   [255. 255. 255.]\n   [255. 255. 255.]]\n\n  ...\n\n  [[255. 255. 255.]\n   [255. 255. 255.]\n   [255. 255. 255.]\n   ...\n   [253. 253. 253.]\n   [255. 255. 255.]\n   [247. 247. 247.]]\n\n  [[255. 255. 255.]\n   [255. 255. 255.]\n   [255. 255. 255.]\n   ...\n   [245. 245. 245.]\n   [255. 255. 255.]\n   [255. 255. 255.]]\n\n  [[255. 255. 255.]\n   [255. 255. 255.]\n   [255. 255. 255.]\n   ...\n   [254. 254. 254.]\n   [254. 254. 254.]\n   [255. 255. 255.]]]\n\n\n ...\n\n\n [[[  6.   4.   5.]\n   [  6.   4.   5.]\n   [  7.   5.   6.]\n   ...\n   [  0.   0.   0.]\n   [  7.   7.   7.]\n   [  3.   3.   3.]]\n\n  [[  6.   4.   5.]\n   [  6.   4.   5.]\n   [  7.   5.   6.]\n   ...\n   [  0.   0.   0.]\n   [  8.   8.   8.]\n   [  8.   8.   8.]]\n\n  [[  5.   3.   4.]\n   [  6.   4.   5.]\n   [  7.   5.   6.]\n   ...\n   [  0.   0.   0.]\n   [  0.   0.   0.]\n   [  5.   5.   5.]]\n\n  ...\n\n  [[  6.   5.  11.]\n   [  6.   5.  11.]\n   [  6.   5.  11.]\n   ...\n   [  3.   3.   5.]\n   [  3.   3.   5.]\n   [  3.   3.   5.]]\n\n  [[  7.   6.  12.]\n   [  7.   6.  12.]\n   [  7.   6.  12.]\n   ...\n   [  3.   3.   5.]\n   [  3.   3.   5.]\n   [  3.   3.   5.]]\n\n  [[  8.   7.  13.]\n   [  8.   7.  13.]\n   [  8.   7.  13.]\n   ...\n   [  4.   4.   6.]\n   [  4.   4.   6.]\n   [  4.   4.   6.]]]\n\n\n [[[ 64.  75. 121.]\n   [ 69.  80. 126.]\n   [ 70.  81. 126.]\n   ...\n   [ 56.  60.  87.]\n   [ 55.  59.  86.]\n   [ 55.  59.  86.]]\n\n  [[ 74.  83. 126.]\n   [ 81.  90. 133.]\n   [ 85.  94. 135.]\n   ...\n   [ 59.  63.  90.]\n   [ 58.  62.  89.]\n   [ 58.  62.  89.]]\n\n  [[ 95. 103. 139.]\n   [103. 111. 147.]\n   [111. 120. 153.]\n   ...\n   [ 65.  67.  92.]\n   [ 64.  66.  91.]\n   [ 64.  66.  91.]]\n\n  ...\n\n  [[118.  54.  45.]\n   [123.  59.  50.]\n   [130.  66.  57.]\n   ...\n   [ 52.  47.  88.]\n   [ 53.  48.  89.]\n   [ 53.  48.  89.]]\n\n  [[118.  53.  47.]\n   [124.  60.  51.]\n   [131.  67.  58.]\n   ...\n   [ 55.  48.  89.]\n   [ 55.  48.  90.]\n   [ 55.  48.  90.]]\n\n  [[120.  55.  49.]\n   [126.  61.  55.]\n   [133.  69.  60.]\n   ...\n   [ 56.  49.  90.]\n   [ 55.  48.  90.]\n   [ 55.  48.  90.]]]\n\n\n [[[122.  50.  38.]\n   [119.  47.  35.]\n   [117.  45.  33.]\n   ...\n   [ 29.  24.  20.]\n   [ 28.  23.  19.]\n   [ 28.  23.  19.]]\n\n  [[122.  50.  38.]\n   [120.  48.  36.]\n   [117.  45.  33.]\n   ...\n   [ 29.  24.  20.]\n   [ 28.  23.  19.]\n   [ 28.  23.  19.]]\n\n  [[122.  50.  38.]\n   [120.  48.  36.]\n   [118.  46.  34.]\n   ...\n   [ 29.  24.  20.]\n   [ 28.  23.  19.]\n   [ 28.  23.  19.]]\n\n  ...\n\n  [[219. 116.  81.]\n   [218. 115.  80.]\n   [217. 114.  79.]\n   ...\n   [149.  63.  30.]\n   [148.  62.  29.]\n   [148.  62.  29.]]\n\n  [[220. 117.  82.]\n   [219. 116.  81.]\n   [218. 115.  80.]\n   ...\n   [149.  63.  28.]\n   [149.  63.  28.]\n   [148.  62.  27.]]\n\n  [[221. 118.  83.]\n   [220. 117.  82.]\n   [219. 116.  81.]\n   ...\n   [149.  63.  28.]\n   [149.  63.  28.]\n   [149.  63.  28.]]]], shape=(50, 128, 128, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#check to see if the shape of the training data is correct\n",
    "for image_batch in train_ds:\n",
    "  print(image_batch.shape)\n",
    "  print(image_batch)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(50, 128, 128, 3)\ntf.Tensor(\n[[[[ 0.00392163 -0.00392157 -0.654902  ]\n   [ 0.01176476  0.00392163 -0.64705884]\n   [ 0.01176476  0.00392163 -0.64705884]\n   ...\n   [ 0.30980396  0.32549024 -0.46666664]\n   [ 0.30980396  0.32549024 -0.46666664]\n   [ 0.30980396  0.32549024 -0.46666664]]\n\n  [[ 0.27843142  0.27843142 -0.41176468]\n   [ 0.27843142  0.27843142 -0.41176468]\n   [ 0.28627455  0.28627455 -0.40392154]\n   ...\n   [ 0.30196083  0.3176471  -0.47450978]\n   [ 0.30196083  0.3176471  -0.47450978]\n   [ 0.30196083  0.3176471  -0.47450978]]\n\n  [[ 0.2941177   0.28627455 -0.44313723]\n   [ 0.2941177   0.28627455 -0.44313723]\n   [ 0.30196083  0.2941177  -0.4352941 ]\n   ...\n   [ 0.2941177   0.30980396 -0.4823529 ]\n   [ 0.2941177   0.30980396 -0.4823529 ]\n   [ 0.2941177   0.30980396 -0.4823529 ]]\n\n  ...\n\n  [[ 0.37254906  0.33333337 -0.67058825]\n   [ 0.37254906  0.33333337 -0.67058825]\n   [ 0.37254906  0.33333337 -0.67058825]\n   ...\n   [ 0.36470592  0.3411765  -0.5764706 ]\n   [ 0.37254906  0.34901965 -0.5686275 ]\n   [ 0.37254906  0.34901965 -0.5686275 ]]\n\n  [[ 0.35686278  0.3176471  -0.6862745 ]\n   [ 0.35686278  0.3176471  -0.6862745 ]\n   [ 0.35686278  0.3176471  -0.6862745 ]\n   ...\n   [ 0.36470592  0.3411765  -0.56078434]\n   [ 0.37254906  0.34901965 -0.5529412 ]\n   [ 0.37254906  0.34901965 -0.5529412 ]]\n\n  [[ 0.3411765   0.30196083 -0.7019608 ]\n   [ 0.3411765   0.30196083 -0.7019608 ]\n   [ 0.3411765   0.30196083 -0.7019608 ]\n   ...\n   [ 0.36470592  0.3411765  -0.56078434]\n   [ 0.37254906  0.34901965 -0.5529412 ]\n   [ 0.37254906  0.34901965 -0.5529412 ]]]\n\n\n [[[ 1.          0.9607843   0.9607843 ]\n   [ 1.          0.96862745  0.9764706 ]\n   [ 1.          0.9843137   0.99215686]\n   ...\n   [ 1.          0.9137255   0.9843137 ]\n   [ 1.          0.8980392   0.99215686]\n   [ 1.          0.8901961   0.99215686]]\n\n  [[ 1.          0.96862745  0.9607843 ]\n   [ 1.          0.96862745  0.9764706 ]\n   [ 1.          0.9843137   0.99215686]\n   ...\n   [ 1.          0.94509804  0.9843137 ]\n   [ 1.          0.92941177  1.        ]\n   [ 1.          0.92156863  0.99215686]]\n\n  [[ 1.          0.9764706   0.9607843 ]\n   [ 1.          0.9764706   0.9764706 ]\n   [ 1.          0.9843137   0.99215686]\n   ...\n   [ 0.92156863  0.99215686  0.9843137 ]\n   [ 0.92156863  0.9843137   1.        ]\n   [ 0.92156863  0.9843137   1.        ]]\n\n  ...\n\n  [[ 0.94509804  1.          0.92156863]\n   [ 0.94509804  1.          0.9372549 ]\n   [ 0.94509804  1.          0.9607843 ]\n   ...\n   [ 0.9372549   1.          0.92941177]\n   [ 0.8980392   1.          0.8901961 ]\n   [ 0.85882354  1.          0.8509804 ]]\n\n  [[ 0.9372549   1.          0.9372549 ]\n   [ 0.92941177  1.          0.9529412 ]\n   [ 0.92941177  1.          0.9764706 ]\n   ...\n   [ 0.9607843   1.          0.8901961 ]\n   [ 0.8980392   1.          0.8352941 ]\n   [ 0.85882354  1.          0.8117647 ]]\n\n  [[ 0.92941177  1.          0.9529412 ]\n   [ 0.92941177  1.          0.9607843 ]\n   [ 0.9137255   1.          0.9764706 ]\n   ...\n   [ 0.9607843   1.          0.85882354]\n   [ 0.9137255   1.          0.827451  ]\n   [ 0.85882354  1.          0.78039217]]]\n\n\n [[[ 0.17647064 -0.7254902  -0.78039217]\n   [ 0.18431377 -0.7176471  -0.77254903]\n   [ 0.18431377 -0.69411767 -0.78039217]\n   ...\n   [ 0.79607844 -0.24705881 -0.5686275 ]\n   [ 0.7882353  -0.23921567 -0.5686275 ]\n   [ 0.7882353  -0.23921567 -0.5686275 ]]\n\n  [[ 0.17647064 -0.7019608  -0.7882353 ]\n   [ 0.17647064 -0.7019608  -0.79607844]\n   [ 0.17647064 -0.6862745  -0.7882353 ]\n   ...\n   [ 0.8117647  -0.23137254 -0.5529412 ]\n   [ 0.8039216  -0.2235294  -0.5529412 ]\n   [ 0.8039216  -0.2235294  -0.5529412 ]]\n\n  [[ 0.15294123 -0.6862745  -0.827451  ]\n   [ 0.15294123 -0.6862745  -0.827451  ]\n   [ 0.16078436 -0.6627451  -0.8117647 ]\n   ...\n   [ 0.81960785 -0.20784312 -0.5372549 ]\n   [ 0.81960785 -0.20784312 -0.5372549 ]\n   [ 0.81960785 -0.20784312 -0.5372549 ]]\n\n  ...\n\n  [[-0.6156863  -0.92941177 -0.94509804]\n   [-0.6313726  -0.94509804 -0.9607843 ]\n   [-0.6392157  -0.9529412  -0.96862745]\n   ...\n   [-0.2862745  -0.8745098  -0.92156863]\n   [-0.27843136 -0.8666667  -0.9137255 ]\n   [-0.27843136 -0.8666667  -0.9137255 ]]\n\n  [[-0.6156863  -0.92941177 -0.94509804]\n   [-0.62352943 -0.9372549  -0.9529412 ]\n   [-0.6313726  -0.94509804 -0.9607843 ]\n   ...\n   [-0.2862745  -0.8745098  -0.92156863]\n   [-0.27843136 -0.8666667  -0.9137255 ]\n   [-0.27843136 -0.8666667  -0.9137255 ]]\n\n  [[-0.6156863  -0.92941177 -0.94509804]\n   [-0.62352943 -0.9372549  -0.9529412 ]\n   [-0.6313726  -0.94509804 -0.9607843 ]\n   ...\n   [-0.2862745  -0.8745098  -0.92156863]\n   [-0.27843136 -0.8666667  -0.9137255 ]\n   [-0.27843136 -0.8666667  -0.9137255 ]]]\n\n\n ...\n\n\n [[[ 0.6156863   0.45882356  0.18431377]\n   [ 0.6156863   0.45882356  0.18431377]\n   [ 0.62352943  0.4666667   0.19215691]\n   ...\n   [ 0.70980394  0.6392157   0.39607847]\n   [ 0.70980394  0.6392157   0.3803922 ]\n   [ 0.70980394  0.6392157   0.3803922 ]]\n\n  [[ 0.5921569   0.43529415  0.16078436]\n   [ 0.5921569   0.43529415  0.16078436]\n   [ 0.60784316  0.45098042  0.17647064]\n   ...\n   [ 0.7019608   0.6313726   0.38823533]\n   [ 0.69411767  0.62352943  0.36470592]\n   [ 0.69411767  0.62352943  0.36470592]]\n\n  [[ 0.5764706   0.41960788  0.14509809]\n   [ 0.5764706   0.41960788  0.14509809]\n   [ 0.5921569   0.43529415  0.16078436]\n   ...\n   [ 0.6862745   0.6156863   0.37254906]\n   [ 0.6862745   0.6156863   0.35686278]\n   [ 0.6784314   0.60784316  0.34901965]]\n\n  ...\n\n  [[ 0.4666667   0.38823533  0.17647064]\n   [ 0.45882356  0.3803922   0.1686275 ]\n   [ 0.45098042  0.37254906  0.16078436]\n   ...\n   [-0.08235294 -0.09803921 -0.19999999]\n   [-0.09019607 -0.06666666 -0.1372549 ]\n   [-0.02745098  0.01176476 -0.04313725]]\n\n  [[ 0.45882356  0.3803922   0.1686275 ]\n   [ 0.45098042  0.37254906  0.16078436]\n   [ 0.43529415  0.35686278  0.14509809]\n   ...\n   [ 0.06666672  0.05098045 -0.05098039]\n   [ 0.09803927  0.12156868  0.05098045]\n   [ 0.12156868  0.17647064  0.12156868]]\n\n  [[ 0.45098042  0.37254906  0.16078436]\n   [ 0.4431373   0.36470592  0.15294123]\n   [ 0.427451    0.34901965  0.13725495]\n   ...\n   [ 0.34901965  0.35686278  0.24705887]\n   [ 0.37254906  0.39607847  0.3411765 ]\n   [ 0.30196083  0.35686278  0.30196083]]]\n\n\n [[[ 0.54509807  0.47450984  0.21568632]\n   [ 0.54509807  0.47450984  0.21568632]\n   [ 0.56078434  0.48235297  0.20784318]\n   ...\n   [ 0.4039216   0.35686278  0.24705887]\n   [ 0.4039216   0.35686278  0.24705887]\n   [ 0.41960788  0.37254906  0.26274514]]\n\n  [[ 0.5921569   0.5058824   0.254902  ]\n   [ 0.5921569   0.5058824   0.254902  ]\n   [ 0.5921569   0.5137255   0.23921573]\n   ...\n   [ 0.4039216   0.34901965  0.22352946]\n   [ 0.41176474  0.34901965  0.24705887]\n   [ 0.41960788  0.36470592  0.23921573]]\n\n  [[ 0.6313726   0.5294118   0.26274514]\n   [ 0.6313726   0.5294118   0.26274514]\n   [ 0.6313726   0.5294118   0.26274514]\n   ...\n   [ 0.4039216   0.33333337  0.20000005]\n   [ 0.41176474  0.3411765   0.22352946]\n   [ 0.427451    0.35686278  0.22352946]]\n\n  ...\n\n  [[ 0.7176471   0.5764706   0.18431377]\n   [ 0.7019608   0.56078434  0.1686275 ]\n   [ 0.69411767  0.5529412   0.17647064]\n   ...\n   [ 0.67058825  0.39607847  0.09803927]\n   [ 0.69411767  0.41960788  0.12156868]\n   [ 0.7019608   0.43529415  0.13725495]]\n\n  [[ 0.7254902   0.56078434  0.19215691]\n   [ 0.70980394  0.54509807  0.17647064]\n   [ 0.69411767  0.5294118   0.16078436]\n   ...\n   [ 0.69411767  0.4666667   0.15294123]\n   [ 0.7176471   0.4901961   0.17647064]\n   [ 0.7176471   0.49803925  0.18431377]]\n\n  [[ 0.7254902   0.56078434  0.19215691]\n   [ 0.70980394  0.54509807  0.17647064]\n   [ 0.69411767  0.5294118   0.16078436]\n   ...\n   [ 0.73333335  0.5137255   0.20000005]\n   [ 0.73333335  0.5372549   0.21568632]\n   [ 0.7411765   0.54509807  0.22352946]]]\n\n\n [[[ 0.41176474  0.79607844  0.56078434]\n   [ 0.41176474  0.79607844  0.56078434]\n   [ 0.427451    0.8117647   0.5764706 ]\n   ...\n   [ 0.18431377  0.52156866  0.30196083]\n   [ 0.13725495  0.47450984  0.254902  ]\n   [ 0.09803927  0.43529415  0.21568632]]\n\n  [[ 0.41960788  0.8039216   0.5686275 ]\n   [ 0.41960788  0.8039216   0.5686275 ]\n   [ 0.427451    0.8117647   0.5764706 ]\n   ...\n   [ 0.14509809  0.48235297  0.26274514]\n   [ 0.11372554  0.45098042  0.2313726 ]\n   [ 0.082353    0.41960788  0.20000005]]\n\n  [[ 0.43529415  0.81960785  0.58431375]\n   [ 0.43529415  0.81960785  0.58431375]\n   [ 0.43529415  0.81960785  0.58431375]\n   ...\n   [ 0.13725495  0.47450984  0.254902  ]\n   [ 0.12941182  0.4666667   0.24705887]\n   [ 0.12156868  0.45882356  0.23921573]]\n\n  ...\n\n  [[ 0.3176471   0.6784314   0.45098042]\n   [ 0.30980396  0.67058825  0.4431373 ]\n   [ 0.30980396  0.67058825  0.4431373 ]\n   ...\n   [ 0.05882359  0.33333337  0.14509809]\n   [ 0.05098045  0.32549024  0.13725495]\n   [ 0.03529418  0.30980396  0.12156868]]\n\n  [[ 0.30196083  0.6627451   0.43529415]\n   [ 0.26274514  0.62352943  0.39607847]\n   [ 0.27843142  0.6392157   0.41176474]\n   ...\n   [ 0.04313731  0.30980396  0.12156868]\n   [ 0.05882359  0.32549024  0.13725495]\n   [ 0.06666672  0.33333337  0.14509809]]\n\n  [[ 0.2941177   0.654902    0.427451  ]\n   [ 0.2313726   0.5921569   0.36470592]\n   [ 0.254902    0.6156863   0.38823533]\n   ...\n   [ 0.0196079   0.28627455  0.09803927]\n   [ 0.05882359  0.32549024  0.13725495]\n   [ 0.082353    0.34901965  0.16078436]]]], shape=(50, 128, 128, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#normalize the rgb data to be [0 - 1]\n",
    "train_ds = train_ds.map(lambda x: (x / 127.5) - 1)\n",
    "\n",
    "for image_batch in train_ds:\n",
    "  print(image_batch.shape)\n",
    "  print(image_batch)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the generator model\n",
    "def make_generator():\n",
    "    model = tf.keras.Sequential()\n",
    "    #first dense layer\n",
    "    model.add(layers.Dense(8 * 8 * 256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Reshape((8, 8, 256)))\n",
    "    assert model.output_shape == (None, 8, 8, 256)\n",
    "    #2nd layer -- downsample filters from 256 -> 128\n",
    "    model.add(layers.Conv2DTranspose(128, (4, 4), strides = (1, 1), padding = 'same', use_bias = False))\n",
    "    assert model.output_shape == (None, 8, 8, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    #3rd layer -- increase resolution to 16x16 and 128 filters\n",
    "    model.add(layers.Conv2DTranspose(128, (4, 4), strides = (2, 2), padding = 'same', use_bias = False))\n",
    "    assert model.output_shape == (None, 16, 16, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    #4th layer -- increase resolution to 32x32 and 128 filters\n",
    "    model.add(layers.Conv2DTranspose(128, (4, 4), strides = (2, 2), padding = 'same', use_bias = False))\n",
    "    assert model.output_shape == (None, 32, 32, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    #5th layer -- increase resolution to 64x64 and 128 filters\n",
    "    model.add(layers.Conv2DTranspose(128, (4, 4), strides = (2, 2), padding = 'same', use_bias = False))\n",
    "    assert model.output_shape == (None, 64, 64, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    #6th layer -- increase resolution to 128x128 and 128 filters\n",
    "    model.add(layers.Conv2DTranspose(128, (4, 4), strides = (2, 2), padding = 'same', use_bias = False))\n",
    "    assert model.output_shape == (None, 128, 128, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    #Final layer -- conv2d to 3 filters(RGB)\n",
    "    model.add(layers.Conv2D(3, (3,3), activation='tanh', padding='same'))\n",
    "    assert model.output_shape == (None, 128, 128, 3)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x243c2364a60>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<svg height=\"251.467969pt\" version=\"1.1\" viewBox=\"0 0 257.9275 251.467969\" width=\"257.9275pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-07-14T00:32:15.716934</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.4.1, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 251.467969 \r\nL 257.9275 251.467969 \r\nL 257.9275 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 33.2875 227.589844 \r\nL 250.7275 227.589844 \r\nL 250.7275 10.149844 \r\nL 33.2875 10.149844 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g clip-path=\"url(#pa472db53d9)\">\r\n    <image height=\"218\" id=\"imagef85cd4eb24\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"33.2875\" xlink:href=\"data:image/png;base64,\r\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\" y=\"-9.589844\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m1c861c67e1\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"34.136875\" xlink:href=\"#m1c861c67e1\" y=\"227.589844\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(30.955625 242.188281)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 4250 \r\nQ 1547 4250 1301 3770 \r\nQ 1056 3291 1056 2328 \r\nQ 1056 1369 1301 889 \r\nQ 1547 409 2034 409 \r\nQ 2525 409 2770 889 \r\nQ 3016 1369 3016 2328 \r\nQ 3016 3291 2770 3770 \r\nQ 2525 4250 2034 4250 \r\nz\r\nM 2034 4750 \r\nQ 2819 4750 3233 4129 \r\nQ 3647 3509 3647 2328 \r\nQ 3647 1150 3233 529 \r\nQ 2819 -91 2034 -91 \r\nQ 1250 -91 836 529 \r\nQ 422 1150 422 2328 \r\nQ 422 3509 836 4129 \r\nQ 1250 4750 2034 4750 \r\nz\r\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"68.111875\" xlink:href=\"#m1c861c67e1\" y=\"227.589844\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(61.749375 242.188281)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 1228 531 \r\nL 3431 531 \r\nL 3431 0 \r\nL 469 0 \r\nL 469 531 \r\nQ 828 903 1448 1529 \r\nQ 2069 2156 2228 2338 \r\nQ 2531 2678 2651 2914 \r\nQ 2772 3150 2772 3378 \r\nQ 2772 3750 2511 3984 \r\nQ 2250 4219 1831 4219 \r\nQ 1534 4219 1204 4116 \r\nQ 875 4013 500 3803 \r\nL 500 4441 \r\nQ 881 4594 1212 4672 \r\nQ 1544 4750 1819 4750 \r\nQ 2544 4750 2975 4387 \r\nQ 3406 4025 3406 3419 \r\nQ 3406 3131 3298 2873 \r\nQ 3191 2616 2906 2266 \r\nQ 2828 2175 2409 1742 \r\nQ 1991 1309 1228 531 \r\nz\r\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"102.086875\" xlink:href=\"#m1c861c67e1\" y=\"227.589844\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 40 -->\r\n      <g transform=\"translate(95.724375 242.188281)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2419 4116 \r\nL 825 1625 \r\nL 2419 1625 \r\nL 2419 4116 \r\nz\r\nM 2253 4666 \r\nL 3047 4666 \r\nL 3047 1625 \r\nL 3713 1625 \r\nL 3713 1100 \r\nL 3047 1100 \r\nL 3047 0 \r\nL 2419 0 \r\nL 2419 1100 \r\nL 313 1100 \r\nL 313 1709 \r\nL 2253 4666 \r\nz\r\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-34\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"136.061875\" xlink:href=\"#m1c861c67e1\" y=\"227.589844\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 60 -->\r\n      <g transform=\"translate(129.699375 242.188281)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2113 2584 \r\nQ 1688 2584 1439 2293 \r\nQ 1191 2003 1191 1497 \r\nQ 1191 994 1439 701 \r\nQ 1688 409 2113 409 \r\nQ 2538 409 2786 701 \r\nQ 3034 994 3034 1497 \r\nQ 3034 2003 2786 2293 \r\nQ 2538 2584 2113 2584 \r\nz\r\nM 3366 4563 \r\nL 3366 3988 \r\nQ 3128 4100 2886 4159 \r\nQ 2644 4219 2406 4219 \r\nQ 1781 4219 1451 3797 \r\nQ 1122 3375 1075 2522 \r\nQ 1259 2794 1537 2939 \r\nQ 1816 3084 2150 3084 \r\nQ 2853 3084 3261 2657 \r\nQ 3669 2231 3669 1497 \r\nQ 3669 778 3244 343 \r\nQ 2819 -91 2113 -91 \r\nQ 1303 -91 875 529 \r\nQ 447 1150 447 2328 \r\nQ 447 3434 972 4092 \r\nQ 1497 4750 2381 4750 \r\nQ 2619 4750 2861 4703 \r\nQ 3103 4656 3366 4563 \r\nz\r\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-36\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"170.036875\" xlink:href=\"#m1c861c67e1\" y=\"227.589844\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 80 -->\r\n      <g transform=\"translate(163.674375 242.188281)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 2216 \r\nQ 1584 2216 1326 1975 \r\nQ 1069 1734 1069 1313 \r\nQ 1069 891 1326 650 \r\nQ 1584 409 2034 409 \r\nQ 2484 409 2743 651 \r\nQ 3003 894 3003 1313 \r\nQ 3003 1734 2745 1975 \r\nQ 2488 2216 2034 2216 \r\nz\r\nM 1403 2484 \r\nQ 997 2584 770 2862 \r\nQ 544 3141 544 3541 \r\nQ 544 4100 942 4425 \r\nQ 1341 4750 2034 4750 \r\nQ 2731 4750 3128 4425 \r\nQ 3525 4100 3525 3541 \r\nQ 3525 3141 3298 2862 \r\nQ 3072 2584 2669 2484 \r\nQ 3125 2378 3379 2068 \r\nQ 3634 1759 3634 1313 \r\nQ 3634 634 3220 271 \r\nQ 2806 -91 2034 -91 \r\nQ 1263 -91 848 271 \r\nQ 434 634 434 1313 \r\nQ 434 1759 690 2068 \r\nQ 947 2378 1403 2484 \r\nz\r\nM 1172 3481 \r\nQ 1172 3119 1398 2916 \r\nQ 1625 2713 2034 2713 \r\nQ 2441 2713 2670 2916 \r\nQ 2900 3119 2900 3481 \r\nQ 2900 3844 2670 4047 \r\nQ 2441 4250 2034 4250 \r\nQ 1625 4250 1398 4047 \r\nQ 1172 3844 1172 3481 \r\nz\r\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"204.011875\" xlink:href=\"#m1c861c67e1\" y=\"227.589844\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 100 -->\r\n      <g transform=\"translate(194.468125 242.188281)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 794 531 \r\nL 1825 531 \r\nL 1825 4091 \r\nL 703 3866 \r\nL 703 4441 \r\nL 1819 4666 \r\nL 2450 4666 \r\nL 2450 531 \r\nL 3481 531 \r\nL 3481 0 \r\nL 794 0 \r\nL 794 531 \r\nz\r\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"237.986875\" xlink:href=\"#m1c861c67e1\" y=\"227.589844\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 120 -->\r\n      <g transform=\"translate(228.443125 242.188281)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_8\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"mbdda104e12\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mbdda104e12\" y=\"10.999219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(19.925 14.798438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mbdda104e12\" y=\"44.974219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(13.5625 48.773438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mbdda104e12\" y=\"78.949219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 40 -->\r\n      <g transform=\"translate(13.5625 82.748437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-34\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mbdda104e12\" y=\"112.924219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 60 -->\r\n      <g transform=\"translate(13.5625 116.723438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-36\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mbdda104e12\" y=\"146.899219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 80 -->\r\n      <g transform=\"translate(13.5625 150.698438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mbdda104e12\" y=\"180.874219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 100 -->\r\n      <g transform=\"translate(7.2 184.673438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mbdda104e12\" y=\"214.849219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 120 -->\r\n      <g transform=\"translate(7.2 218.648438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 33.2875 227.589844 \r\nL 33.2875 10.149844 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 250.7275 227.589844 \r\nL 250.7275 10.149844 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 33.2875 227.589844 \r\nL 250.7275 227.589844 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 33.2875 10.149844 \r\nL 250.7275 10.149844 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pa472db53d9\">\r\n   <rect height=\"217.44\" width=\"217.44\" x=\"33.2875\" y=\"10.149844\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "#generate an image\n",
    "generator = make_generator()\n",
    "\n",
    "noise = tf.random.normal([1, 100])\n",
    "image = generator(noise, training=False)\n",
    "image = (image + 1) / 2.0\n",
    "\n",
    "plt.imshow(image[0,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN based discriminator\n",
    "def make_discriminator():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same', input_shape=[128, 128, 3]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tf.Tensor([[0.04636339]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#classify our generated image\n",
    "discriminator = make_discriminator()\n",
    "decision = discriminator(image)\n",
    "print(decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#discriminator loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def disc_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def gen_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "#optimizers\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint setup\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 300\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# You will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training step\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    #print(images.shape)\n",
    "\n",
    "    noise = tf.random.normal([BATCH_SIZE, 100])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "\n",
    "        generated_images = generator(noise, training=True)\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        generator_loss = gen_loss(fake_output)\n",
    "        discrimiator_loss = disc_loss(real_output, fake_output)\n",
    "\n",
    "        gen_gradients = gen_tape.gradient(generator_loss, generator.trainable_variables)\n",
    "        disc_gradients = disc_tape.gradient(discrimiator_loss, discriminator.trainable_variables)\n",
    "\n",
    "        generator_optimizer.apply_gradients(zip(gen_gradients, generator.trainable_variables))\n",
    "        discriminator_optimizer.apply_gradients(zip(disc_gradients, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  for epoch in range(epochs):\n",
    "    print(\"Epoch \" + str(epoch) + \" started\")\n",
    "    start = time.time()\n",
    "\n",
    "    batch = 1\n",
    "    for image_batch in dataset:\n",
    "        \n",
    "        #print(\"Batch \" + str(batch) + \" started\")\n",
    "        #print(image_batch.shape)\n",
    "        train_step(image_batch)\n",
    "\n",
    "        \n",
    "        #print(\"Batch \" + str(batch) + \" finished\")\n",
    "        batch += 1\n",
    "\n",
    "    # Produce images for the GIF as you go\n",
    "    generate_and_save_images(generator,\n",
    "                             epoch + 1,\n",
    "                             seed)\n",
    "\n",
    "    # Save the model every 15 epochs\n",
    "    if (epoch + 1) % 15 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "  # Generate after the final epoch\n",
    "  generate_and_save_images(generator,\n",
    "                           epochs,\n",
    "                           seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "  predictions = model(test_input, training=False)\n",
    "  predictions = (predictions + 1) / 2.0\n",
    "\n",
    "  fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(4, 4, i+1)\n",
    "      plt.imshow(predictions[i, :, :, :])\n",
    "      plt.axis('off')\n",
    "\n",
    "  plt.savefig('./generated_images/image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0 started\n",
      "Time for epoch 1 is 20.00649881362915 sec\n",
      "Epoch 1 started\n",
      "Time for epoch 2 is 13.63751769065857 sec\n",
      "Epoch 2 started\n",
      "Time for epoch 3 is 13.772008180618286 sec\n",
      "Epoch 3 started\n",
      "Time for epoch 4 is 13.78901720046997 sec\n",
      "Epoch 4 started\n",
      "Time for epoch 5 is 13.95397686958313 sec\n",
      "Epoch 5 started\n",
      "Time for epoch 6 is 13.837007999420166 sec\n",
      "Epoch 6 started\n",
      "Time for epoch 7 is 13.820016860961914 sec\n",
      "Epoch 7 started\n",
      "Time for epoch 8 is 14.091983795166016 sec\n",
      "Epoch 8 started\n",
      "Time for epoch 9 is 13.899612188339233 sec\n",
      "Epoch 9 started\n",
      "Time for epoch 10 is 13.977969408035278 sec\n",
      "Epoch 10 started\n",
      "Time for epoch 11 is 14.006999492645264 sec\n",
      "Epoch 11 started\n",
      "Time for epoch 12 is 14.053001642227173 sec\n",
      "Epoch 12 started\n",
      "Time for epoch 13 is 14.152026176452637 sec\n",
      "Epoch 13 started\n",
      "Time for epoch 14 is 14.08899998664856 sec\n",
      "Epoch 14 started\n",
      "Time for epoch 15 is 14.503973722457886 sec\n",
      "Epoch 15 started\n",
      "Time for epoch 16 is 14.193999290466309 sec\n",
      "Epoch 16 started\n",
      "Time for epoch 17 is 14.336993217468262 sec\n",
      "Epoch 17 started\n",
      "Time for epoch 18 is 14.677992582321167 sec\n",
      "Epoch 18 started\n",
      "Time for epoch 19 is 14.304066896438599 sec\n",
      "Epoch 19 started\n",
      "Time for epoch 20 is 14.359986066818237 sec\n",
      "Epoch 20 started\n",
      "<ipython-input-15-55152b6feb2d>:7: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = plt.figure(figsize=(4, 4))\n",
      "Time for epoch 21 is 14.26800012588501 sec\n",
      "Epoch 21 started\n",
      "Time for epoch 22 is 14.386014938354492 sec\n",
      "Epoch 22 started\n",
      "Time for epoch 23 is 14.413984775543213 sec\n",
      "Epoch 23 started\n",
      "Time for epoch 24 is 14.994987487792969 sec\n",
      "Epoch 24 started\n",
      "Time for epoch 25 is 14.443013906478882 sec\n",
      "Epoch 25 started\n",
      "Time for epoch 26 is 14.526999950408936 sec\n",
      "Epoch 26 started\n",
      "Time for epoch 27 is 14.330013513565063 sec\n",
      "Epoch 27 started\n",
      "Time for epoch 28 is 14.463985681533813 sec\n",
      "Epoch 28 started\n",
      "Time for epoch 29 is 14.38901424407959 sec\n",
      "Epoch 29 started\n",
      "Time for epoch 30 is 14.52174711227417 sec\n",
      "Epoch 30 started\n",
      "Time for epoch 31 is 12.820611238479614 sec\n",
      "Epoch 31 started\n",
      "Time for epoch 32 is 13.105008363723755 sec\n",
      "Epoch 32 started\n",
      "Time for epoch 33 is 15.08811092376709 sec\n",
      "Epoch 33 started\n",
      "Time for epoch 34 is 14.478485822677612 sec\n",
      "Epoch 34 started\n",
      "Time for epoch 35 is 14.523002862930298 sec\n",
      "Epoch 35 started\n",
      "Time for epoch 36 is 14.440997838973999 sec\n",
      "Epoch 36 started\n",
      "Time for epoch 37 is 14.46401596069336 sec\n",
      "Epoch 37 started\n",
      "Time for epoch 38 is 14.447985172271729 sec\n",
      "Epoch 38 started\n",
      "Time for epoch 39 is 14.422999858856201 sec\n",
      "Epoch 39 started\n",
      "Time for epoch 40 is 14.497000932693481 sec\n",
      "Epoch 40 started\n",
      "Time for epoch 41 is 14.522706031799316 sec\n",
      "Epoch 41 started\n",
      "Time for epoch 42 is 14.533601760864258 sec\n",
      "Epoch 42 started\n",
      "Time for epoch 43 is 15.061002254486084 sec\n",
      "Epoch 43 started\n",
      "Time for epoch 44 is 14.421118974685669 sec\n",
      "Epoch 44 started\n",
      "Time for epoch 45 is 14.724833726882935 sec\n",
      "Epoch 45 started\n",
      "Time for epoch 46 is 14.431998491287231 sec\n",
      "Epoch 46 started\n",
      "Time for epoch 47 is 14.439028263092041 sec\n",
      "Epoch 47 started\n",
      "Time for epoch 48 is 14.470681190490723 sec\n",
      "Epoch 48 started\n",
      "Time for epoch 49 is 14.444002628326416 sec\n",
      "Epoch 49 started\n",
      "Time for epoch 50 is 14.65000033378601 sec\n",
      "Epoch 50 started\n",
      "Time for epoch 51 is 14.491000175476074 sec\n",
      "Epoch 51 started\n",
      "Time for epoch 52 is 14.640002727508545 sec\n",
      "Epoch 52 started\n",
      "Time for epoch 53 is 14.720002889633179 sec\n",
      "Epoch 53 started\n",
      "Time for epoch 54 is 14.53502869606018 sec\n",
      "Epoch 54 started\n",
      "Time for epoch 55 is 14.616018295288086 sec\n",
      "Epoch 55 started\n",
      "Time for epoch 56 is 15.42898416519165 sec\n",
      "Epoch 56 started\n",
      "Time for epoch 57 is 14.453997373580933 sec\n",
      "Epoch 57 started\n",
      "Time for epoch 58 is 14.636988401412964 sec\n",
      "Epoch 58 started\n",
      "Time for epoch 59 is 14.668011665344238 sec\n",
      "Epoch 59 started\n",
      "Time for epoch 60 is 14.848062515258789 sec\n",
      "Epoch 60 started\n",
      "Time for epoch 61 is 14.59637999534607 sec\n",
      "Epoch 61 started\n",
      "Time for epoch 62 is 14.716978073120117 sec\n",
      "Epoch 62 started\n",
      "Time for epoch 63 is 14.89200210571289 sec\n",
      "Epoch 63 started\n",
      "Time for epoch 64 is 14.87002444267273 sec\n",
      "Epoch 64 started\n",
      "Time for epoch 65 is 14.617162704467773 sec\n",
      "Epoch 65 started\n",
      "Time for epoch 66 is 14.378995656967163 sec\n",
      "Epoch 66 started\n",
      "Time for epoch 67 is 14.218976259231567 sec\n",
      "Epoch 67 started\n",
      "Time for epoch 68 is 14.183382511138916 sec\n",
      "Epoch 68 started\n",
      "Time for epoch 69 is 14.151004076004028 sec\n",
      "Epoch 69 started\n",
      "Time for epoch 70 is 14.168012142181396 sec\n",
      "Epoch 70 started\n",
      "Time for epoch 71 is 14.064000368118286 sec\n",
      "Epoch 71 started\n",
      "Time for epoch 72 is 15.029980421066284 sec\n",
      "Epoch 72 started\n",
      "Time for epoch 73 is 13.898276567459106 sec\n",
      "Epoch 73 started\n",
      "Time for epoch 74 is 14.101992130279541 sec\n",
      "Epoch 74 started\n",
      "Time for epoch 75 is 14.44699764251709 sec\n",
      "Epoch 75 started\n",
      "Time for epoch 76 is 14.298023223876953 sec\n",
      "Epoch 76 started\n",
      "Time for epoch 77 is 14.098989725112915 sec\n",
      "Epoch 77 started\n",
      "Time for epoch 78 is 13.382981777191162 sec\n",
      "Epoch 78 started\n",
      "Time for epoch 79 is 12.763028860092163 sec\n",
      "Epoch 79 started\n",
      "Time for epoch 80 is 12.488015174865723 sec\n",
      "Epoch 80 started\n",
      "Time for epoch 81 is 12.651999950408936 sec\n",
      "Epoch 81 started\n",
      "Time for epoch 82 is 12.690977573394775 sec\n",
      "Epoch 82 started\n",
      "Time for epoch 83 is 12.559022665023804 sec\n",
      "Epoch 83 started\n",
      "Time for epoch 84 is 12.685975790023804 sec\n",
      "Epoch 84 started\n",
      "Time for epoch 85 is 12.827776908874512 sec\n",
      "Epoch 85 started\n",
      "Time for epoch 86 is 12.710084199905396 sec\n",
      "Epoch 86 started\n",
      "Time for epoch 87 is 12.83000922203064 sec\n",
      "Epoch 87 started\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-d2e0a4733c0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-ac1275365049>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataset, epochs)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m#print(\"Batch \" + str(batch) + \" started\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m#print(image_batch.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(train_ds, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(epoch_no):\n",
    "  return PIL.Image.open('./generated_images/image_at_epoch_{:04d}.png'.format(epoch_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(EPOCHS)"
   ]
  }
 ]
}